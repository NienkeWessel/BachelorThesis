

\section{Introduction}
In the next two (?) chapters of this thesis, we will look at subsets of Rust. Of these subsets, we will define the syntax and will try to form a semantics as well. With these semantics, we will try to prove some interesting properties. This first chapter will go into detail at every step. Later chapters will not do this anymore and assume that what was mentioned here is known to the reader.

The subset of Rust that interests us in this chapter will be dubbed as `move only', which is also the name of this chapter. In this subset of Rust, you can only assign variables. If you do so, the ownership of the resource will be moved to that variable. You cannot borrow and nothing is mutable. That means everything is constant. While this does not seem like a very useful language, it will provide a good base to work from when adding new features.

We will first look at the syntax of our `move only' language.

\section{Syntax}
\begin{definition}
A statement $S$ is defined recursively by:
$$S ::= \textrm{skip} \mid S_1; S_2 \mid a: \textrm{let } x:\tau \textrm{ in } S' \mid x := e$$
where $e$ is an expression as defined below, $\tau$ is a type as defined below and $S_1$, $S_2$ and $S'$ are again statements.
\end{definition}

\begin{definition}
An expression $e$ is defined recursively by:
$$e ::= x \mid i \mid e_1 + e_2$$
\end{definition}

\begin{definition}
A type $\tau$ is
$$\tau ::= \textrm{Int}$$
\end{definition}

This syntax is very simple, but enough for us to say something useful about ownership. We use only one data type, as more data types will add no additional interesting facts for ownership. However, this syntax can be expanded to include more types. 

In particular, take note of the syntax for let. We split a statement such as \verb|let x = 5| in \verb|let x in (x = 5)| (brackets added for clearity). This can be done for every type of let-statement. It was done to show that there are actually two steps in a statement such as \verb|let x = 5|. First of all, the variable \verb|x| is declared. Then, a value is assigned to this \verb|x|. 

After the splitting (when necessary), we've 'desugared' the statement in a similar way as was done in \ref{desugar}. This is done to make lifetimes explicit. That means we change \verb|let x in (x = 5)| to \verb|a: {let x in (x = 5)}|. We do not add the \verb|'| for the \verb|a|, as we will not be talking about generics and this only complicates the syntax. Lastly, we desugar more, to also include the type $\tau$, so that the previous statement becomes \verb|a: {let x:i32 in (x = 5)}|. 

In order to illustrate this, let's write the following program (taken from \ref{desugar}) in our syntax: 

\begin{minted}[linenos, frame=lines]{rust}
let x = 0;
let y = x;
let z = y;
\end{minted}

\begin{minted}[linenos, frame=lines]{rust}
a: {
    let x: Int in x = 0;
    b: {
        let y: Int in y = x;
    }
}
\end{minted}

This looks a lot like the desugaring in \ref{desugar}, but has no \verb|'| to indicate lifetimes, in order to simplify the syntax.


\section{Semantics: Framework}
In this chapter we will look at two types of semantics. The first is a big step semantics and the second a small steps semantics. Both are based on the exposition in Nielson and Nielson [citation needed]. 

Our big step semantics is called `natural semantics'. Its rules are of the form: 

$$\langle S, s \rangle \to s'$$

Where $S$ is a statement as defined in the previous section. $s$ and $s'$ are states, for which an exact definition will be given shortly. 

Our small step semantics is called `structural operational semantics'. Its rules are of the form:

$$\langle S, L, s \rangle \Rightarrow \langle S', L', s' \rangle$$

Where $S$, $S'$ and $s$, $s'$ are again statements and states respectively. $L$ and $L'$ are lists of program parts, which will also be defined later on. 

In order to work with these rules, we need some more mathematical definitions of different kinds of sets and functions we are interested in. Therefore, we will first define a framework in this section, and then move on to the actual rules in the next two sections. 

In this section we will need to distinguish actual definitions from pseudo-definitions. The latter means we give an intended interpretation for a symbol or a set in natural language. This is done to show what the symbol or set is supposed to represent in the real world. However, the actual meaning of the symbol or set will depend on how we will use it in other definitions later on. They are added nonetheless to help the reader navigate through the many difficult notations introduced in the chapter and to give them some intuition to what is happening. To distinguish between actual definitions and these pseudo-definitions, the latter is preceded by `Informal', as below. 

\subsection*{Variable values}
The symbols in this subsection will be defined informally.  

\begin{infdefinition}
We will need the following symbols 
\begin{enumerate}[noitemsep, label={\roman*)}]
    \item $-$ will be used to indicate that a variable has not been declared at all.
    \item $\perp$ will be used to indicate that a variable has been declared using \verb|let|, but has not been assigned a value.
\end{enumerate}
\end{infdefinition}

Intuitively, these are some special kind of values a variable can have. 

As is conventional, the set $\mathbb{Z}$ will be used to denote the set of integers. We extent $\mathbb{Z}$ with two new symbols mentioned in the previous section, to get $\mathbb{Z}_{ext}$. Therefore: 

\begin{definition}
$\mathbb{Z}_{ext} := \mathbb{Z} ~\cup ~ \{\perp, -\}$.
\end{definition}

\subsection*{Expressions}
\begin{definition}
We have the following sets:
\begin{enumerate}[noitemsep, label={\roman*)}]
    \item \textbf{Var} denotes the set of all variables in Rust.
    \item \textbf{Num} denotes the set of all numbers in the form of how they are represented in Rust.
    \item \textbf{Add} denotes the set of all tuples from \textbf{Exp}, i.e. $\textbf{Add} = \textbf{Exp} \times \textbf{Exp}$ . 
    \item $\textbf{Exp} = \textbf{Num} \cup \textbf{Var} \cup \textbf{Add}$
\end{enumerate}
\end{definition}

Notice that we have a recursive definition of \textbf{Exp}. \textbf{Exp} should be interpreted as a representation for all possible syntactic expressions. Note that elements from \textbf{Add} will usually be denoted as ``$e_1 + e_2$'' instead of ``$(e_1, e_2)$''.

As \textbf{Exp} is a recursive set, it will turn out to be useful to also be able to gather all the variables that are in an expression $e$. We do this by defining a new function $\mathcal{V}$.

\begin{definition}
We define the function $\mathcal{V}: \textbf{Exp} \to \mathcal{P}(\textbf{Var})$ recursively by:
\begin{align*}
    \mathcal{V}(i)          &= \emptyset
\\  \mathcal{V}(x)          &= \{ x \}
\\  \mathcal{V}(e_1 + e_2)  &= \mathcal{V}(e_1) \cup \mathcal{V}(e_2)
\end{align*}
\end{definition}

We see that this definition recursively walks through the expression to gather all variable mentions and join them together into a single set. 

Now we need a way to go from a Rust representation for a number to an actual number.

\begin{definition}
$\mathcal{N}$: $\textbf{Num} \to \mathbb{Z}$ translates a Rust representation for a number to the actual number
\end{definition}

We leave the details of this function out, as it is cumbersome and not relevant for the remainder of this thesis. We will simply assume that we have such a function and that it behaves as we would expect. For a detailed exposition on this function, we refer the reader to Nielson and Nielson [citation needed]. 

\subsection*{State and evaluation}
The state is a function that maps a variable to a value. Therefore, we can define the set \textbf{State} which contains all possible states. 

\begin{definition}
\textbf{State} is the set of all functions $s: \textbf{Var} \to \mathbb{Z}_{ext}$
\end{definition}

We also have an evaluation functions $\mathcal{A}$, which gives for an element from \textbf{Exp} and a state $s$ the value. Here, value is an element of $\mathbb{Z}_{ext}$

\begin{definition}
The evaluation function $\mathcal{A}: \textbf{Exp} \times \textbf{State} \to \mathbb{Z}_{ext}$ is defined by:
\begin{align*}
    \letterfunc{A}{i}s          &= \letterfunc{N}{i}
\\  \letterfunc{A}{x}s          &= s(x)
\\  \letterfunc{A}{e_1 + e_2}s  &= \letterfunc{A}{e_1}s + \letterfunc{A}{e_2}s \textrm{ if } \letterfunc{A}{e_1}s \in \mathbb{Z} \textrm{ and } \letterfunc{A}{e_2}s \in \mathbb{Z}
\\  \letterfunc{A}{e_1 + e_2}s  &= - \textrm{ otherwise}
\end{align*}
\end{definition}

We now have sufficient apparatus to continue with the actual semantic rules. 

\section{Semantics: Big step}
As mentioned earlier on, our big step semantics are called natural semantics with rules of the form 
$$\langle S, s \rangle \to s'$$

Where $S$ is a statement and $s$, $s'$ are states. We now know what that means, so we can start to define our actual rules. 

\subsection{Natural semantics rules}

\begin{definition} 
We define the following natural semantics rules (name on the left)

\begin{tabular}{p{5em}p{18em}p{13em}}
[skip$_{\textrm{ns}}$] &
\centering$\langle$ \texttt{skip} $, s \rangle \to s$ & \medskip\\

[comp$_{\textrm{ns}}$] &
\centering \AxiomC{$\langle S_1, s \rangle \to s' $}
\AxiomC{$\langle S_2, s' \rangle \to s''$}
\BinaryInfC{$\langle S_1$; $S_2, s \rangle \to s''$}
\DisplayProof \medskip& \\

[let$_{\textrm{ns}}$] &
\centering
\AxiomC{$\langle S, s[x\mapsto \perp] \rangle \to s'$}
\UnaryInfC{$\langle a : \texttt{let x } : \tau \texttt{ in } S, s \rangle \to s'[x \mapsto s(x)]$}
\DisplayProof \medskip& \\

[ass$_{\textrm{ns}}$] &
\centering$\langle$ \texttt{x := } $e, s \rangle \to s[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-]$ & if $\letterfunc{A}{x}s = \perp$, $\letterfunc{A}{e}s \neq \perp$ and $\letterfunc{A}{e}s \neq -$\medskip\\
\end{tabular} 
Where $\mathcal{V}(e)\mapsto-$ is an abbreviation for `for all $x \in \mathcal{V}(e)$, $x \mapsto -$'.
\end{definition} 

The rules for skip and composition are assumed to be self-explanatory. The rule for let is derived from the idea that a let-statement should put the variable \verb|x| at $\perp$ (declared but not assigned). When the body of a let-statement is finished, \verb|x| should be put at whatever it was before we encountered the let-statement. 

As for the assignment rule, there are several conditions that need to apply before we can consider the rule. If one of these conditions does not apply, we cannot apply this rule and that means we cannot make a derivation of the program. The program is incorrect, then. The first rule states that \verb|x| must have been declared, but not assigned. The second and third rules state that the expression must result in a value. This is important, in for example the following program: \verb|a: let x in (x = y)|. Here, \verb|y| has not been given a value (it has not even been declared), so we cannot rightly assign \verb|y|s value to \verb|x|. 

Note that these rules are important to model Rust in the best possible way. It is not possible to assign to a value that has not been declared and it is also not possible to assign a value to a non-mutable variable that already has a value. 

\subsection{Properties of our natural semantics rules}
In this subsection, we will look at and prove some interesting properties of our natural semantics. 

\subsubsection*{Determinism}

The first will be that our program is deterministic. This means that if we run a program multiple times with the same starting state, the end state will always be the same. This is of course a desirable property in most programs.

\begin{theorem}
For every statement $S$, every state $s, s', s''$, if $\langle S, s \rangle \to s'$ and $\langle S, s \rangle \to s''$, then $s' = s''$.
\end{theorem}

\begin{proof}
The proof proceeds by induction on the structure of $S$. We distinguish the following cases:
\begin{itemize}[noitemsep]
    \item $S$ = \texttt{skip} : if $\langle \texttt{skip}, s \rangle \to s'$ and $\langle \texttt{skip}, s \rangle \to s''$, then this can only be achieved by [skip$_{\textrm{ns}}$], which means $s=s'$ and $s=s''$, so $s' = s''$. 
    \item Blah \emph{details will be filled in later, currently don't have the proof with me and enough other things to focuss on first}.
\end{itemize}
\end{proof}

\subsubsection*{Variable allocation}

While this first theorem is one that is generally considered useful to prove, the next is very specific to Rust and the way we defined our semantics. It states that a program no longer has any variables in memory after it terminates. 

\begin{theorem}
For every statement $S$, every state $s, s'$, if $\langle S, s \rangle \to s'$, then for all variables $x$, if $\letterfunc{A}{x}s = -$ then $\letterfunc{A}{x}s' = -$.
\end{theorem}

\begin{proof}
The proof proceeds by induction on the structure of $S$. We distinguish the following cases:
\begin{itemize}[noitemsep]

    \item $S$ = \texttt{skip} : if $\langle \texttt{skip}, s \rangle \to s'$, then this can only be achieved by [skip$_{\textrm{ns}}$], which means $s=s'$. Therefore if $\letterfunc{A}{x}s = -$ then $\letterfunc{A}{x}s' = -$. 
    
    \item $S$ = \texttt{x := } $e$ : assume $\langle \texttt{x := } e, s \rangle \to s'$. Then the only rule that can have been applied is [ass$_{\textrm{ns}}$], so  $s'$ must be $s[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-]$ and $\letterfunc{A}{x}s = \perp$, $\letterfunc{A}{e}s \neq \perp$ and $\letterfunc{A}{e}s \neq -$. If one of these things had not been the case, we could not have applied any rules, and therefore our assumption `$\langle S, s \rangle \to s'$' would be false. So we can assume these things. Now we look at a variable $y$, chosen randomly. We distinguish two cases:
    \begin{itemize}
        \item $x=y$ : if $\letterfunc{A}{x}s \not = -$ there is nothing to prove. So assume $\letterfunc{A}{x}s = -$. However, this is contradictory to our assumption. So this is not possible.
        \item $x\not = y$ : we again assume $\letterfunc{A}{y}s = -$, as otherwise there is nothing to prove. Then we can again distinguish two cases: 
        \begin{itemize}
            \item $y \in \mathcal{V}(e)$ \footnote{Note: in practice, it will not be possible for $y$ to be both an element of $\mathcal{V}(e)$ and $\letterfunc{A}{y}s = -$, as one of the assumptions of the rule [ass$_{\textrm{ns}}$] assumes $\letterfunc{A}{e}s \neq -$. However, we would need to prove this with induction on the shape of an expression and it turns out not to be necessary as the proof is rather simple anyways.} : in $s'$, we have $\mathcal{V}(e)\mapsto-$, so $y \mapsto -$, which means $\letterfunc{A}{y}s = -$.
            \item $y \not \in \mathcal{V}(e)$ : as $x \not = y$ and $y \not \in \mathcal{V}(e)$, the assignment of $y$ is not any different from $s$ in $s'=s[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-]$, so $\letterfunc{A}{y}s' = -$
        \end{itemize}
    \end{itemize}
    This exhausts all possible cases, so this proves the case $S$ = \texttt{x := } $e$.
    
    \item  $S$ = a : \texttt{let x } : $\tau$ \texttt{ in } $S$ : assume $\langle a : \texttt{let x } : \tau \texttt{ in } $S$, s \rangle \to s'$. Then the rule that has been aplied last must be [let$_{\textrm{ns}}$]. This has the following derivation tree \AxiomC{$\langle S, s[x\mapsto \perp] \rangle \to s''$}
\UnaryInfC{$\langle a : \texttt{let x } : \tau \texttt{ in } S, s \rangle \to s''[x \mapsto s(x)]$}
\DisplayProof
. We again look at a random variable $y$ and assume $\letterfunc{A}{y}s' = -$ (otherwise we have nothing to prove). We again distinguish two cases:
    \begin{itemize}
        \item $x=y$ : then $s' = s''[x \mapsto s(x)]$. As $\letterfunc{A}{x}s = -$ and $x$ in $s''$ maps to $s(x)$, we can conclude $\letterfunc{A}{x}s'' = -$
        \item $x\neq y$ : we are now interested in $\letterfunc{A}{y}s''$. However, we can apply the induction hypothesis to $\langle S, s[x\mapsto \perp] \rangle \to s''$ as $\letterfunc{A}{y}s[x\mapsto \perp] = \letterfunc{A}{y}s = -$. So $\letterfunc{A}{y}s'' = -$. As $s' = s''[x \mapsto s(x)]$, which changes nothing to the value of $y$, we can conclude $\letterfunc{A}{y}s' = -$
    \end{itemize}
    \item $S$ = $S_1$; $S_2$ : assume $\langle S_1$; $S_2, s \rangle \to s'$ (Note the accents $'$). Then the last rule that has been applied must be [comp$_{\textrm{ns}}$], so that gives the following derivation tree \AxiomC{$\langle S_1, s \rangle \to s'' $}
\AxiomC{$\langle S_2, s'' \rangle \to s'$}
\BinaryInfC{$\langle S_1$; $S_2, s \rangle \to s'$}
\DisplayProof .
Now assume $x$ to be a random variable and $\letterfunc{A}{x}s = -$. We can apply the induction hypothesis to $\langle S_1, s \rangle \to s''$, so we know $\letterfunc{A}{x}s'' = -$. We can then again apply the induction hypothesis to $\langle S_2, s'' \rangle \to s'$. So we know $\letterfunc{A}{x}s' = -$, which was what was to be proven.
\end{itemize} 
This exhausts all possible shapes of our statement $S$, and hence proves the theorem.
\end{proof}

Note that this means that after a program terminates, none of the results are available anymore. To make use of the data, you would have to write it to a file or some other IO device. This is a significant break from the exposition of Nielson and Nielson [citation needed] where the state of a terminated program shows the values of the variables at the end and you can thus `access' the results of the calculations. However, as this is not how a computer normally works, we have chosen to not do this. 


\section{Semantics: Small step}
We now move onto a small step semantics. We use mostly the same framework as in the big step semnatics, unless stated otherwise. Our small step semantics, as stated above, has rules of the form $\langle S, L, s \rangle \Rightarrow \langle S', L', s' \rangle$, where $S$, $S'$ are statements and $s$, $s'$ are states as defined above. To properly define what kind of things $L$ and $L'$ are, we first need to define what we mean by `a program instruction'. 

\begin{definition}
A program instruction $I$ has the following form:
$$I ::= S \mid (x,v)$$
where $S$ is a statement, $x$ a variable from \textbf{Var} and $v$ an element from $\mathbb{Z}_{ext}$.
\end{definition}

Now we can look at our \emph{lists}:

\begin{definition}
\label{lists}
A L is a list then, L is of the form 
\begin{itemize}[noitemsep]
    \item L = \texttt{Nil}
    \item L = I::L', where I is a program instruction and L' is again a list. 
\end{itemize}
\end{definition}

So now we can state that our $L$ and $L'$ from the rule should be lists as defined in definition \ref{lists}. We will use this list as a sort of stack to keep track of program parts that we will want to execute later on. 

\subsection{Operational semantics rules}
\begin{definition} 
\label{os}
We define the following semantic derivation rules (name on the left):

\begin{tabular}{p{5em}p{18em}p{13em}}
[load$_{\textrm{os}}$] &
\centering$\langle$ \texttt{skip} $, I::L, s \rangle \Rightarrow \langle I, L, s \rangle$ & \medskip\\

[comp$_{\textrm{os}}$] &
\centering$\langle S_1; S_2, L, s \rangle \Rightarrow \langle S_1, S_2::L, s \rangle$ & \medskip\\

[ass$_{\textrm{os}}$] &
\centering $\langle \texttt{x := } e, L, s \rangle \Rightarrow \langle \texttt{skip}, L, s[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-] \rangle$ & \medskip\\

[let$_{\textrm{os}}$] &
\centering $\langle a : \texttt{let x } : \tau \texttt{ in } S, L, s \rangle \Rightarrow \langle S, (x,s(x))::L, s[x\mapsto \perp] \rangle$ & \medskip\\

[free$_{\textrm{os}}$] &
\centering$\langle (x,v), L, s \rangle \Rightarrow \langle \texttt{skip}, L, s[x\mapsto v] \rangle$ & \medskip\\
\end{tabular} 
Where $\mathcal{V}(e)\mapsto-$ is an abbreviation for `for all $x \in \mathcal{V}(e)$, $x \mapsto -$'.
\end{definition} 
Note that $\langle \texttt{skip}, \texttt{Nil}, s \rangle$ has no derivation; this is the/an end state. 

We will be using $\langle S, L, s \rangle \Rightarrow ^{*} \langle S', L', s' \rangle$ to indicate we can go from $\langle S, L, s \rangle$ to $\langle S', L', s' \rangle$ in zero or more steps. We will be using $\langle S, L, s \rangle \Rightarrow ^{n} \langle S', L', s' \rangle$ with $n$ a natural number to indicate we can go from $\langle S, L, s \rangle$ to $\langle S', L', s' \rangle$ in exactly $n$ steps. In both cases, a step is one of the rules from definition \ref{os} .


\subsection{Properties of our semantic rules}

\subsubsection*{Determinism}

\begin{theorem}
For every statement $S$, every state $s, s', s''$, every list $L$, if $\langle S, L, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, L, s' \rangle$ and $\langle S, L, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, L, s'' \rangle$, then $s' = s''$.
\end{theorem}

\begin{proof}
The proof proceeds by induction on the structure of $S$. We distinguish the following cases:
\begin{itemize}[noitemsep]
    \item $S$ = \texttt{skip} : 
    \item Blah \emph{details will be filled in later, shouldn't be too hard I hope...}.
\end{itemize}
\end{proof}

\subsubsection*{Concerning lists and compositions}
First, we will see that it is irrelevant what is in the list for a statement currently executing. In order to prove that, we will need to prove a lemma that states that if a composition evaluates in a certain amount of steps to a \texttt{skip}, this must be done by first evaluating $S_1$, then loading $S_2$ and evaluating it. 

\begin{lemma}
\label{breakingdowncomp}
For every statement $S_1$, $S_2$, for every list $L$, for every state $s$, $s'$, if $\langle S_1; S_2, L, s \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $L, s' \rangle$, then it must be the case that $\langle S_1;S_2, L, s \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $S_2::L, s'' \rangle \Rightarrow \langle S_2, L, s'' \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $L, s' \rangle$ for some state $s''$. 
\end{lemma}

\begin{proof}
Assume $\langle S_1; S_2, L, s \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $L, s' \rangle$. Now look at $\langle S_1; S_2, L, s \rangle$. We know there must be a derivation. The only possible rule is [comp$_{\textrm{os}}$], so we have $\langle S_1; S_2, L, s \rangle \Rightarrow \langle S_1, S_2::L, s \rangle$. Now we see that our list has grown by one item. However we know there is a derivation and it ends with the list $L$, instead of $S_2::L$. So at some point in the derivation the list must be reduced again. The only possible rule that could have done that is [load$_{\textrm{os}}$]. So we must have $\langle \texttt{skip} , S_2::L, s'' \rangle \Rightarrow \langle S_2, L, s'' \rangle$ at some point in the derivation. Therefore we have $\langle S_1; S_2, L, s \rangle \Rightarrow \langle S_1, S_2::L, s \rangle \Rightarrow ^{*} \langle \texttt{skip} , S_2::L, s'' \rangle \Rightarrow \langle S_2, L, s'' \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $L, s' \rangle$, which was to be proven.
\end{proof}

We will look at a similar lemma for the let statement:

\begin{lemma}
\label{breakingdownlet}
For every statement $S$, for every list $L$, for every state $s$, $s'$, if $\langle a : \texttt{let x } : \tau \texttt{ in } S , L, s \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $L, s' \rangle$, then it must be the case that $\langle a : \texttt{let x } : \tau \texttt{ in } S, L, s \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $(x,s(x))::L, s'' \rangle \Rightarrow \langle (x,s(x)), L, s'' \rangle \Rightarrow \langle$ \texttt{skip}, $L, s' \rangle$ for some state $s''$. 
\end{lemma}

\begin{proof}
The proof is similar to lemma \ref{breakingdowncomp}. 
\end{proof}

Now we will move on, to proof that for evaluating a statement, it is irrelevant what is in the list. 

\begin{proposition}
For every program instruction $S$, every state $s$, $s'$, if $\langle S, \texttt{Nil}, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$, the for all lists $L$, we have $\langle S, L, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, L, s' \rangle$. 
\end{proposition}

\begin{proof}
We know that there must be a derivation from $\langle S, \texttt{Nil}, s \rangle$ to $\langle \texttt{\textrm{skip}}, \texttt{Nil}, s' \rangle$, so $S$ must be of one of the forms of definition \ref{os}. We therefore proof the proposition by induction on the form of the program instruction $S$.

\begin{itemize}
    \item $S = \texttt{skip}$ : then $\langle \texttt{skip}, L, s \rangle$ is already of the correct form, so we can evaluate to the desired state in zero steps. 
    
    \item $S = \texttt{x := } e$ : assume $\langle \texttt{x := } e, \texttt{Nil}, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$. Then we know the rule that must have been applied next is [ass$_{\textrm{os}}$], so $\langle \texttt{x := } e, \texttt{Nil}, s \rangle \Rightarrow \langle \texttt{skip}, \texttt{Nil}, s[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-] \rangle$, which is of the desired form. So it must be that $s' = s[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-]$. Now we look at $\langle \texttt{x := } e, L, s \rangle$. The only rule we can apply is [ass$_{\textrm{os}}$], so we get $\langle \texttt{x := } e, L, s \rangle \Rightarrow \langle \texttt{skip}, L, s$ $[x \mapsto \letterfunc{A}{e}s][\mathcal{V}(e)\mapsto-] \rangle$, which is of the desired form, and has the right $s'$, which proves this case. 
    
    \item $S = (v,e)$ : assume $\langle (v,e), \texttt{Nil}, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$. Then we know the rule that must have been applied next was [free$_{\textrm{os}}$]. So $\langle (v,e), \texttt{Nil}, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s[x\mapsto v] \rangle$. This is of the desired form, so it must be the case that $s' = s[x\mapsto v]$. Now we look at $\langle (v,e), L, s \rangle$. The only rule that can be applied is [free$_{\textrm{os}}$], so we get $\langle (v,e), L, s \rangle \Rightarrow  \langle \texttt{skip}, L, s[x\mapsto v] \rangle$, which is of the desired form, and has the right $s'$, which proves this case.
    
    \item $S = S_1; S_2$ : assume $\sos{ S_1;S_2}{\texttt{Nil}}{ s} \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$. Then by [comp$_{\textrm{os}}$] and lemma \ref{breakingdowncomp}, we have $\langle S_1;S_2, \texttt{Nil}, s \rangle \Rightarrow \langle S_1, S_2::\texttt{Nil}, s \rangle \Rightarrow ^{*} \langle$ \texttt{skip}, $S_2::\texttt{Nil}, s'' \rangle \Rightarrow \langle S_2, \texttt{Nil}, s'' \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$ ( * ). Now we look at $\langle S_1;S_2, L, s \rangle$ with $L$ random. We can apply the rule [comp$_{\textrm{os}}$] to see $\langle S_1;S_2, L, s \rangle \Rightarrow \langle S_1, S_2::L, s \rangle$. Now we can apply the induction hypothesis to $\langle S_1, S_2::L, s \rangle$, as we have $\langle S_1, \texttt{Nil}, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s'' \rangle$ by ( * ). So we get $\langle S_1, S_2::L, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, S_2::L, s'' \rangle$. Then by [load$_{\textrm{os}}$], $\langle \texttt{skip}, S_2::L, s'' \rangle \Rightarrow \langle S_2 , L , s''\rangle$. Now we can apply the induction hypothesis to $\langle S_2 , L , s''\rangle$, as we have $\langle S_2, \texttt{Nil}, s''\rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s'\rangle$ by ( * ). We get $\langle S_2 , L , s''\rangle \Rightarrow ^{*} \langle \texttt{skip}, L, s'\rangle$. That means we can form the derivation sequence $\langle S_1;S_2, L, s \rangle \Rightarrow \langle S_1, S_2::L, s \rangle \Rightarrow ^{*} \langle \texttt{skip}, S_2::L, s'' \rangle \Rightarrow \langle S_2 , L , s''\rangle \Rightarrow ^{*} \langle \texttt{skip}, L, s'\rangle$. This gives us $\langle S_1;S_2, L, s \rangle  \Rightarrow ^{*} \langle \texttt{skip}, L, s'\rangle$, which was to be proven. 
        
    \item $S = a : \texttt{let x } : \tau \texttt{ in } S'$ : assume $\sos{a : \texttt{let x } : \tau \texttt{ in } S'}{\texttt{Nil}}{s} \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$. There is only one rule that could have been applied first, [let$_{\textrm{os}}$], which gives us $\langle a : \texttt{let x } : \tau \texttt{ in } S', \texttt{Nil}, s \rangle \Rightarrow \langle S, (x,s(x))::\texttt{Nil}, s[x\mapsto v] \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$. By lemma \ref{breakingdownlet} we get $\langle a : \texttt{let x } : \tau \texttt{ in } S', \texttt{Nil}, s \rangle \Rightarrow \langle S, (x,s(x))::\texttt{Nil}, s[x\mapsto v] \rangle \Rightarrow ^{*} \langle \texttt{skip}, (x,s(x))::\texttt{Nil}, s'' \rangle \Rightarrow ^{*} \langle \texttt{skip}, \texttt{Nil}, s' \rangle$. Now we look at $\langle a : \texttt{let x } : \tau \texttt{ in } S', L, s \rangle$ for some list $L$. Then we can apply [let$_{\textrm{os}}$] to get $\langle a : \texttt{let x } : \tau \texttt{ in } S', L, s \rangle \Rightarrow \langle S, (x,s(x))::L, s[x\mapsto v] \rangle$. We can now apply the induction hypothesis, as we know \emph{Okay we have some problems here as the list is not empty.... Change theorem to maybe state that if it is possible for any L, then true for all L?}
\end{itemize}
Hence we can conclude that the theorem holds. 
\end{proof}

Intuitively, this makes sense. If we look at the rules, we see that we can only move something from the list to the front if the front only contains a \texttt{skip} statement, and that thus everything that had been there, must have been finished evaluating. 

We will now do a simple proof to show that in a composition, the execution of $S_1$ does not depend on what $S_2$ entails. 

\begin{proposition}
For every statement $S_1$, $S_2$, for every list $L$, for every state $s$, $s'$ and for every natural number $n$, if $\sos{S_1}{L}{s} \Rightarrow ^{n} \langle \texttt{\textrm{skip}}, L, s \rangle$, then $\langle S_1;S_2, L, s \rangle$ $\Rightarrow ^{n+2} \langle S_2, L, s \rangle$
\end{proposition}

\begin{proof}
Assume $\langle S_1, L, s \rangle \Rightarrow ^{n} \langle \texttt{skip}, L, s \rangle$ and look at $\langle S_1;S_2, L, s \rangle$. Then $\langle S_1;S_2, L, s \rangle$ $\Rightarrow _{[\textrm{comp}$_{\textrm{os}}$]} $\langle S_1, S_2::L, s \rangle$
\end{proof}

\section{Compile time check}
To model the compile time checker of Rust, we need something that passes through the code and checks the code as it goes. It should then return whether the code is correct or not. 

We use a reduced state like function to keep track of the stuff

\begin{definition}
\textbf{RState} is the set of functions $r: \textbf{Var} \to \{-, \perp, \star \}$
\end{definition}

The interpretation of $\star$ here is that the variable has a value. However, for compile time checking, it is irrelevant what that value is, so we leave that out. We just want to know that the variable has been assigned a certain value.

\begin{definition}
\label{compiletimechecker}
The \emph{compile time checker} is a derivation system that has the following rules
\begin{align*}
\cc{\texttt{skip}}{\texttt{Nil}}{r} & \to \texttt{true}  \\
\cc{\texttt{skip}} {P::L}{ r}       & \to \cc{P}{L}{r}  \\
\cc{S_1; S_2}{L}{r}                 & \to \cc{S_1}{S_2::L}{r}  \\
\cc{x:=e}{L}{r}                     & \to \cc{\texttt{skip}}{L}{r[x\mapsto \star][ev(e) \mapsto -]} \\
                                    & \textrm{if }r(x) = \perp \textrm{ and } \forall y \in ev(e), r(y) = \star \\
                                    & \to \texttt{false} \textrm{ otherwise}\\
\cc{a : \texttt{let x } : \tau \texttt{ in } S}{L}{r} & \to \cc{S}{(x,r(x))::L}{r[x\mapsto \perp]} \\
\cc{(x,v)}{L}{r}                    & \to \cc{\texttt{skip}}{L}{r[x \mapsto v]}
\end{align*}
\end{definition}

Now first of all, we want this checker to always finish, no matter what the program it needs to check is. It would be impractical if a checker would not terminate when compiling a program, because the compilation would take infinitely long. In order to prove this, we will asign a (nonnegative) `length' to every possible input of the checker and show that with every step the checker takes, this `length' decreases. This in turn gives us a decreasing sequence of nonnegative numbers. This sequence must always be finite, since there is no infinite decreasing sequence of nonnegative numbers. 

\begin{theorem}
The compile checker from definition \ref{compiletimechecker} always terminates.
\end{theorem}

\begin{proof}
\end{proof}

%\begin{definition}
%TODO::: Fix function signature
%The compilte time check function $\mathcal{C}: \times \textbf{List} \times \textbf{RState} \to %\textbf{RState} \cup \{ \texttt{false} \}$ is defined by:
%\begin{align*}
%    \mathcal{C}( \texttt{skip}, \texttt{Nil}, r )          &= r
%\\  \mathcal{C}( \texttt{skip}, P::L, r )         &= \mathcal{C}( P, L, r)
%\\ \mathcal{C}( S_1; S_2, L, r )   &= \mathcal{C}(S_1, S_2::L, r)
%\\ \mathcal{C}(x:=e, L, r )   &= \mathcal{C}(\texttt{skip}, L, r[x\mapsto \star][ev(e) \mapsto -] ) 
%\\ & \textrm{ if } r(x) = \perp \textrm{ and } \forall y \in ev(e),~s(y) = \star
%\\ & = \texttt{false} \textrm{ otherwise}
%\\ \mathcal{C}( a : \texttt{let x } : \tau \texttt{ in } S, L, r )   &= \mathcal{C}( S, %(x,r(x))::L, r[x\mapsto \perp] ) 
%\\ \mathcal{C}( (x,v), L, r )          &= \mathcal{C}( \texttt{skip}, L, r[x \mapsto v]) 
%\end{align*}
%$\mathcal{C}( S, L, r )$ is \texttt{false} is all other cases. 
%\end{definition}

%\begin{theorem}
%If $\mathcal{C}( S, L, r) = r'$ for some $r' \in \textbf{RState}$, then 
%\end{proposition}

%\begin{proof}
%Assume $\langle S_1, L, s \rangle \Rightarrow ^{n} \langle \texttt{skip}, L, s \rangle$ and look at $\langle S_1;S_2, L, s \rangle$. Then $\langle S_1;S_2, L, s \rangle$ $\Rightarrow _{[\textrm{comp}$_{\textrm{os}}$]} $\langle S_1, S_2::L, s \rangle$
%\end{proof}

%pdflatex -synctex=1 -interaction=nonstopmode --shell-escape main.tex

